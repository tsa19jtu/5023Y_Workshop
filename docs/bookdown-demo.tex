% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Introduction to Bioinformatics - 5023Y},
  pdfauthor={Philip Leftwich},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter

\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{block}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{block}{note}}
  {\end{block}}
\newenvironment{rmdtip}
  {\begin{block}{tip}}
  {\end{block}}
\newenvironment{rmdwarning}
  {\begin{block}{warning}}
  {\end{block}}
  
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{caption}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Introduction to Bioinformatics - 5023Y}
\author{Philip Leftwich}
\date{2021-08-17}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This is a very short introduction to some of the processes and tools we use when working in bioinformatics.

This is not meant to be anywhere near comprehensive, you will be shown a few of the fundamentals, and one walkthrough of a single bioinformatics `pipeline'.

The \emph{main} difference between Bioinformatics and the data analysis you have done before is the \textbf{size} of the data. Bioinformatics usually deals with sequencing data, and this data has large file sizes.

Handling \textbf{big data} means you need to know how to operate on a supercomputer so we will be learning a little bit of Linux, processing \textbf{big} data into \textbf{small} data and then exporting into R to make some more amazing data visuals and interpret our findings!

\textbf{DON'T} worry if you don't understand everything

\textbf{DO} ask lots of questions!

\hypertarget{Unix}{%
\chapter{Unix}\label{Unix}}

Unix is very likely the most fundamental skillset we can develop for bioinformatics (and much more than bioinformatics). Many of the most common and powerful bioinformatics approaches happen in this text-based environment, and having a solid foundation here can make everything we're trying to learn and do much easier. This is a short introductory tutorial to help us get from being completely new to Unix up to being friendly with it ðŸ™‚

\hypertarget{what-is-unixlinux}{%
\section{What is Unix/Linux?}\label{what-is-unixlinux}}

UNIX is a computer operating system. It was first developed in 1969 at Bell Labs. Unix is written in the programming language \texttt{C}.

Unix is proprietary software, whereas Linux is \emph{basically} free and open-source Unix.

The Linux Operating System is highly flexible, free, open-source (like R) and uses very little RAM to run (Unlike Windows OS) - as such you find most supercomputers run on Linux. Operationally Linux is almost identical to Unix, and so we often refer to it under the umbrella term of ``unix-Like'' systems.

\hypertarget{some-terms}{%
\subsection{Some terms}\label{some-terms}}

Here are some terms worth knowing, don't worry about memorising them, it can just be useful to have these to refer to in the future.

\begin{longtable}{ll}
\toprule
\textbf{Term} & \textbf{What it is}\\
\midrule
shell & what we use to talk to the computer; anything where you are pointing and clicking with a mouse is aÂ GraphicalÂ UserÂ Interface (GUI) shell; something with text only is aÂ CommandÂ LineÂ Interface (CLI) shell\\
command line & a text-based environment capable of taking input and providing output\\
Terminal & A program that runs a shell\\
Unix & a family of operating systems (we also use the term â€œUnix-likeâ€ because one of the most popular operating systems derived from Unix is specifically named asÂ notÂ being Unix)\\
Linux & a "Unix-like" OS\\
\addlinespace
bash & the most common programming language used at a Unix command-line\\
flag & a way to set options for a function, a specific type of argument usually preceded by a -\\
\bottomrule
\end{longtable}

\begin{quote}
\textbf{Note}

You should be very familiar with using a GUI (RStudio), but remember we have spent a lot of time working with files and directories using the the command line (CLI) in R. This is useful practice, because most supercomputers lack a GUI, you must work entirely using the command line.
\end{quote}

\hypertarget{why-learn-unix}{%
\section{Why Learn Unix?}\label{why-learn-unix}}

Most sequencing data files are large, and require a lot of RAM to process. As a result most of the work Bioinformaticians do is not hosted on their own computers, instead they ``remote-connect'' to high performance supercomputers or cluster computers. Almost all of these high performance computers use ``Unix-like'' operatings systems, the most common of which is Linux.

As stated above Linux is free (so no expensive licenses), open-source so lots of developers, its also well known for being stable, secure, reliable and efficient.

You already have some experience with using a Linux OS - every time you log into RStudio Cloud you are connecting to a supercomputer that runs on Linux. Normally we do not interact directly with the OS, instead we use R and RStudio directly.

But when you click on the RStudio Terminal it provides direct access to a command-line where we can execute commands and functions directly in Linux.

This allows us to start using programs other than R, and potentially use multiple programs \& programming languages to work together.

\begin{quote}
\textbf{Note}

This series of practicals is designed for you to have a first introduction to Bioinformatics, it's about exposure, not memorising or mastering anything. Don't worry about the details!
\end{quote}

\hypertarget{getting-started}{%
\section{Getting started}\label{getting-started}}

Before we get started we need a terminal to work in.

\begin{itemize}
\item
  Open the Bioinformatics RStudio Cloud Project in the 5023Y workspace
\item
  Click on the \texttt{Terminal} tab next to \texttt{Console} in the bottom-left pane of the RStudio GUI, this opens a command-line \emph{Shell}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/terminal} 

}

\caption{Here is an example of the Terminal tab, right next to the R console}\label{fig:unnamed-chunk-4}
\end{figure}

\begin{itemize}
\item
  This is our ``command line'' where we will be typing all of our commands. We type our commands in \texttt{bash}
\item
  The \$ is where you start typing from, left of this it tells you what folder you are currently in (\texttt{working\ directory})
\item
  If you need to, you can exit the Terminal and start a new session easily with options in RStudio
\end{itemize}

\hypertarget{a-few-foundational-rules}{%
\section{A few foundational rules}\label{a-few-foundational-rules}}

\begin{itemize}
\item
  Just like in R spaces are special, spaces break things apart, as a rule it is therefore better to have functions and file names with dashes (-) or underscores (\_) - e.g.~``draft\_v3.txt'' is preferred to ``draft v3.txt''.
\item
  The general syntax on the command line is: \texttt{command\ argument}. Again this is very similar to R except we don't use brackets e.g.~in R we are used to \texttt{command(argument)}
\item
  Arguments can be \textbf{optional} e.g.~if their is a default argument you may not have to write anything. Some functions \emph{require} that arguments are specified. Again this is just like R.
\end{itemize}

\hypertarget{lets-get-started}{%
\section{Let's get started}\label{lets-get-started}}

We will perform a very simple function and get a flavour of the similarities and differences to working in \texttt{R}.

\texttt{date} is a command that prints out the date and time. Copy and paste this command into your terminal

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{date}
\end{Highlighting}
\end{Shaded}

This prints out the date/time in UTC

More information on using the date function (here){[}\url{https://www.geeksforgeeks.org/date-command-linux-examples/}{]}

We can also ask for the output for a particular timezone using the \texttt{TZ} function and \texttt{date}

\begin{Shaded}
\begin{Highlighting}[]
    \VariableTok{TZ=}\NormalTok{Europe/London }\FunctionTok{date}
\end{Highlighting}
\end{Shaded}

Or we can ask the computer what the date will be next Tuesday\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{date} \AttributeTok{{-}{-}date}\OperatorTok{=}\StringTok{"next tue"}
\end{Highlighting}
\end{Shaded}

\hypertarget{downloading-data}{%
\subsection{Downloading data}\label{downloading-data}}

We will start by typing in an instruction to download data from an online data repository, unpack the contents and inspect it:

\begin{itemize}
\item
  curl is a command line tool for transferring data to and from the server here we will use this to download data from an online repository.
\item
  tar will \emph{unpack} the data from a compressed file format
\item
  cd change the directory so we \emph{land} in the new folder we have made
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
    \ExtensionTok{curl} \AttributeTok{{-}L} \AttributeTok{{-}o}\NormalTok{ unix\_intro.tar.gz https://ndownloader.figshare.com/files/15573746}
    \FunctionTok{tar} \AttributeTok{{-}xzvf}\NormalTok{ unix\_intro.tar.gz }\KeywordTok{\&\&} \FunctionTok{rm}\NormalTok{ unix\_intro.tar.gz}
    \BuiltInTok{cd}\NormalTok{ unix\_intro}
\end{Highlighting}
\end{Shaded}

\begin{rmdwarning}
Check each command line has run, in the example above you might find
that the first two lines run, to download and unpack data, while the
last line to change directory doesn't run until you hit enter
\end{rmdwarning}

\hypertarget{more-functions}{%
\subsection{More functions}\label{more-functions}}

Unlike date, most commands require arguments and won't work without them. head is a command that prints the first lines of a file, so it requires us to provide the file we want it to act on:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head}\NormalTok{ example.txt}
\end{Highlighting}
\end{Shaded}

Here ``example.txt'' is the required argument, and in this case it is also what's known as a positional argument. Whether things need to be provided as positional arguments or not depends on how the command or program we are using was written.

Sometimes we need to specify the input file by putting something in front of it (e.g.~some commands will use the -i flag, but it's often other things as well).

\textbf{Q. What's in the text file? - Click here for Answer}

\emph{Pretty boring, each line contains the text ``This is line'' followed by the line number e.g.}

\emph{- This is line 1}

\emph{- This is line 2}

\emph{etc.}

There are also optional arguments for the head command. The default for head is to print the first 10 lines of a file. We can change that by specifying the -n flag, followed by how many lines we want:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head} \AttributeTok{{-}n}\NormalTok{ 5 example.txt}
\end{Highlighting}
\end{Shaded}

How would we know we needed the -n flag for that? There are a few ways to find out. Many standard Unix commands and other programs will have built-in help menus that we can access by providing --help as the only argument:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head} \AttributeTok{{-}{-}help}
\end{Highlighting}
\end{Shaded}

Again this is very similar to the logic in which R commands are strucutred e.g.~\texttt{??ggplot()} The synatx is similar even if the specific icons or arguments are different.

Remember just like with R, one of your best friends is Google! As you get familiar with any language or OS we might remember a few flags or specific options, but searching for options and details when needed is definitely the norm!

\hypertarget{unix-file-structure}{%
\section{Unix File Structure}\label{unix-file-structure}}

There are two special locations in all Unix-based systems: the ``root'' location and the current user's ``home'' location. ``Root'' is where the address system of the computer starts; ``home'' is usally where the current user's location starts.

Just to be awkward RStudio Cloud actually has us working in a different location ``Cloud'', which is underneath Root but separate to home which would be in the ``Users'' folder.

\begin{center}\includegraphics[width=0.8\linewidth]{images/file_system_structure} \end{center}

We tell the command line where files and directories are located by providing their address, their ``path''. If we use the pwd command (for print working directory), we can find out what the path is for the directory we are sitting in.

\begin{Shaded}
\begin{Highlighting}[]
    \BuiltInTok{pwd}
\end{Highlighting}
\end{Shaded}

And if we use the ls command (for list), we can see what directories and files are in the current directory we are sitting in.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ls}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\textbf{Note}

Why is it important to know this? Usually when you are working on a Unix-like environment there is no GUI (nice-click and point interface), all commands have to be submitted through the terminal. So you would have to get used to navigating directories with typed commands, and it's useful to know what the standard hierarchy is.
\end{quote}

\hypertarget{absolute-vs-relative-file-paths}{%
\section{Absolute vs relative file paths}\label{absolute-vs-relative-file-paths}}

You should be used to these concepts from your work with R projects.

There are two ways to specify the path (address) of the file we want to do something to:

\begin{itemize}
\item
  An \textbf{absolute path} is an address that starts from an explicitly specified location: usually the ``root'' \texttt{/} or the ``home'' \texttt{\textasciitilde{}/} location. (Side note, because we also may see or hear the term, the ``full path'', is usually the absolute path that starts from the ``root'' /.)
\item
  A \textbf{relative path} is an address that starts from wherever we are currently sitting (the working directory). For example, let's look again at the head command we ran above:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head}\NormalTok{ example.txt}
\end{Highlighting}
\end{Shaded}

What we are actually doing here is using a relative path to specify where the ``example.txt'' file is located. This is because the command line \textbf{automatically looks} in the current working directory if we don't specify anything else about its location.

We can also run the same command on the same file using an \textbf{absolute path} - note Rstudio cloud has a slightly unique set-up in that we start from a folder designated cloud:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head}\NormalTok{ /cloud/project/unix\_intro/example.txt}
\end{Highlighting}
\end{Shaded}

The previous two commands both point to the same file right now. But the first way, head example.txt, will only work if we are entering it while ``sitting'' in the directory that holds that file, while the second way will work no matter where we happen to be in the computer.

It is \textbf{important to always think about where} we are in the computer when working at the command line. One of the most common errors/easiest mistakes to make is trying to do something to a file that isn't where we think it is. Let's run head on the ``example.txt'' file again, and then let's try it on another file: ``notes.txt'':

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ example.txt}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{ notes.txt}
\end{Highlighting}
\end{Shaded}

Here the head command works fine on ``example.txt'', but we get an error message when we call it on ``notes.txt'' telling us no such file or directory. If we run the ls command to list the contents of the current working directory, we can see the computer is absolutely right -- spoiler alert: it usually is -- and there is no file here named ``notes.txt''.

The ls command by default operates on the current working directory if we don't specify any location, but we can tell it to list the contents of a different directory by providing it as a positional argument:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ls}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ls}\NormalTok{ experiment}
\end{Highlighting}
\end{Shaded}

We can see the file we were looking for is located in the subdirectory called ``experiment''. Here is how we can run head on ``notes.txt'' by specifying an accurate relative path to that file:

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head}\NormalTok{ experiment/notes.txt}
\end{Highlighting}
\end{Shaded}

\hypertarget{moving-around}{%
\section{Moving around}\label{moving-around}}

We can also move into the directory containing the file we want to work with by using the \texttt{cd} command (\textbf{c}hange \textbf{d}irectory). This command takes a positional argument that is the path (address) of the directory we want to change into. This can be a relative path or an absolute path. Here we'll use the relative path of the subdirectory, ``experiment'', to change into it

\begin{Shaded}
\begin{Highlighting}[]
    \BuiltInTok{cd}\NormalTok{ experiment/}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \BuiltInTok{pwd}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ls}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{head}\NormalTok{ notes.txt}
\end{Highlighting}
\end{Shaded}

Great. But now how do we get \textbf{back ``up''} to the directory above us? One way would be to provide an absolute path, like \texttt{cd\ /cloud/project/unix\_intro}, but there is also a handy shortcut. \texttt{..} which are special characters that act as a relative path specifying ``up'' one level -- one directory -- from wherever we currently are.

So we can provide that as the positional argument to cd to get back to where we started:

\begin{Shaded}
\begin{Highlighting}[]
    \BuiltInTok{cd}\NormalTok{ ..}
\end{Highlighting}
\end{Shaded}

Moving around the computer like this might feel a bit cumbersome and frustrating at first, but after spending a little time with it, you will get used to it, and it starts to feel more natural.

\begin{quote}
\textbf{Note}

One way to speed things up is to start using \textbf{tab} to perform \textbf{tab-completion} often this will auto-complete file names! Press tab twice quickly and it will print all possible combinations.
\end{quote}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

While maybe not all that exciting, these things really are the foundation needed to start utilizing the command line -- which then gives us the capability to use lots of tools that only work at a command line, manipulate large files rapidly, access and work with remote computers, and more! These are the fundamental tools that every scientist needs to work with \textbf{big data}.

\hypertarget{terms}{%
\subsection{Terms}\label{terms}}

\begin{longtable}{ll}
\toprule
\textbf{Term} & \textbf{What it is}\\
\midrule
path & the address system the computer uses to keep track of files and directories\\
root & where the address system of the computer starts,Â /\\
home & where the current userâ€™s location starts,Â \textasciitilde{}/\\
absolute path & an address that starts from a specified location, i.e. root, or home\\
relative path & an address that starts from wherever we are\\
\addlinespace
tab-completion & our best friend\\
\bottomrule
\end{longtable}

\hypertarget{commands}{%
\subsection{Commands}\label{commands}}

\begin{longtable}{ll}
\toprule
\textbf{Command} & \textbf{What it is}\\
\midrule
date & prints out information about the current date and time\\
head & prints out the first lines of a file\\
pwd & prints out where we are in the computer (printÂ workingÂ directory)\\
ls & lists contents of a directory (list)\\
cd & changeÂ directories\\
\bottomrule
\end{longtable}

\hypertarget{special-characters}{%
\subsection{Special characters}\label{special-characters}}

\begin{longtable}{ll}
\toprule
\textbf{Command} & \textbf{What it is}\\
\midrule
Characters & Meaning\\
/ & the computerâ€™s root location\\
\textasciitilde{}/ & the userâ€™s home location\\
../ & specifies a directory one level â€œaboveâ€ the current working directory\\
\bottomrule
\end{longtable}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

You won't get used to operating in bash, or moving around directories using just the command line in a single session. So if you think you are interested in developing your bioinformatic skills, spend some time practising.

Here is a link to a couple of extended tutorials you can bookmark if you want to explore this further:

\url{https://datacarpentry.org/shell-genomics/01-introduction/index.html}

\hypertarget{stretch-yourself---optional-extras-to-try-a-couple-of-other-skills}{%
\section{Stretch yourself - optional extras to try a couple of other skills}\label{stretch-yourself---optional-extras-to-try-a-couple-of-other-skills}}

\hypertarget{creation}{%
\subsection{Creation}\label{creation}}

I want to create a new directory to store some code files I'm going to write later, so I'll use \texttt{mkdir} to create a new directory called Code:

\textbf{Check you are in the \texttt{unix\_intro} folder - Click here for Answer}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{pwd}
\end{Highlighting}
\end{Shaded}

\textbf{Make a new directory called Code - Click here for Answer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mkdir}\NormalTok{ Code}
\end{Highlighting}
\end{Shaded}

\textbf{Check this folder has been created using a list function}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ls}
\end{Highlighting}
\end{Shaded}

Note that I used a relative file path to create the Code directory - but I could have also specified an absolute filepath to generate that folder in whatever location I want.

There are a few ways to make new files on the command line. The simplest is to generate a blank file with the \texttt{touch} command followed by the path (relative or absolute) to the file you want to create

\textbf{Make a new file called data-science-class.txt - Click here for Answer}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{touch}\NormalTok{ data{-}science{-}class.txt}
\FunctionTok{ls} \AttributeTok{{-}l}
\end{Highlighting}
\end{Shaded}

\emph{Note here I could just use ls to list all files and folders in a directory, but if i set the flag \texttt{-l} then it will produce a \textbf{l}ong list of files.}

\emph{If the entry in the first column is a \texttt{d}, then the row in the table corresponds to a directory, otherwise the information in the row corresponds to a file.}

\emph{The string of characters following the \texttt{d} in the case of a directory or following the first \texttt{-} in the case of a file represent the permissions for that file or directory - I won't cover that here - but some of the links I provide go into more detail.}

\hypertarget{ngs-sequence-analysis}{%
\chapter{NGS sequence analysis}\label{ngs-sequence-analysis}}

\hypertarget{background-on-high-throughput-sequencing}{%
\section{Background on high throughput sequencing}\label{background-on-high-throughput-sequencing}}

High-throughput sequencing, also known as massively parallel sequencing or next-generation sequencing (NGS), is a collection of methods and technologies that can sequence DNA thousands/millions of fragments at a time. The market leader on NGS is Illumina, and an overview of their technology is in the video below.

There are many uses for high throughput sequencing including:

\begin{itemize}
\item
  Whole genome sequencing
\item
  Amplicon sequencing - PCR of a targeted gene(s) is step one

  \begin{itemize}
  \tightlist
  \item
    environmental DNA
  \item
    16S Bacterial community analysis
  \item
    Targeted gene panels
  \end{itemize}
\item
  RNA sequencing
\item
  ChIP sequencing: Protein-DNA interaction analysis
\end{itemize}

Importantly a \textbf{lot} of the basic bioinformatics is the same across these technologies, because the data that is produced from the sequencing run is basically the same as well. The \emph{big} data generated here are all massive \emph{FASTQ} files, processing these follows basically the same initial pipeline for all applications.

\hypertarget{some-terms-1}{%
\section{Some terms}\label{some-terms-1}}

\begin{longtable}{ll}
\toprule
\textbf{Term} & \textbf{What it is}\\
\midrule
Insert & the DNA fragment that is being used for sequencing\\
Read & The part of the insert that is sequenced\\
Single read & A procedure in which the insert is sequenced once\\
Paired end & A procedure in which the insert is sequenced twice, once from each end\\
Flowcell & A small glass chip on which the DNA fragments are attached and then sequenced. The flowcell is covered in probes that allow hybridisation of DNA adpaters that were ligated onto the inserts\\
\addlinespace
Lane & The flowcell has 8 physically separate lanes. Sequencing occurs in parallel on all lanes\\
Multiplexing/Demultiplexing & Sequencing multiple independent biological samples on the same lane is called multiplexing. After sequencing separating the data from from samples that were multiplexed is performed computationally and is called demultiplexing. This is done by a script that identifies unique sequences on the adapters that were attached to reads\\
Pipeline & The series of computational processes used to go from FASTQ data files to results and biological inferences\\
\bottomrule
\end{longtable}

\hypertarget{the-data}{%
\section{The data}\label{the-data}}

This data comes from exploring an underwater mountain \textasciitilde3 km down at the bottom of the Pacific Ocean that serves as a low-temperature (\textasciitilde5-10Â°C) hydrothermal venting site.

This amplicon dataset was generated from DNA extracted from crushed basalts collected from across the mountain with the goal being to begin characterizing the microbial communities of these deep-sea rocks. No one had ever been here before, so this was a broad-level community survey. The sequencing was done on the Illumina MiSeq platform with 2x300 paired-end sequencing using primers targeting the V4 region of the 16S rRNA gene.

There are 20 samples total: 4 extraction ``blanks'' (nothing added to DNA extraction kit), 2 bottom-water samples, 13 rocks, and one biofilm scraped off of a rock. None of these details are important for you to remember, it's just to give some overview if you care.

\textbf{Q. Why would we include ``blank'' samples in our sequencing run? - Click here for Answer}

\emph{This sort of ``environmental data'' is very at risk of contamination, although the DNA extractions, and PCRs have to be run under sterile conditions or they will pick up bacteria from the lab and not the sample. Despite our best efforts we can still get minor contamination, these ``blank'' runs can be useful as anything in these samples} \textbf{cannot} \emph{have come from our deep-sea rocks, and therefore we could choose to ``remove'' sequences that match these in our other samples and label them as contamination.}

In the following figure, overlain on the map are the rock sample collection locations, and the panes on the right show examples of the 3 distinct types of rocks collected: 1) basalts with highly altered, thick outer rinds (\textgreater1 cm); 2) basalts that were smooth, glassy, thin exteriors (\textasciitilde1-2 mm); and 3) one calcified carbonate.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/samples} 

}

\caption{Map of collection sites and examples of the rocks collected}\label{fig:nice-fig}
\end{figure}

Altogether the uncompressed size of the working directory we are downloading here is \textasciitilde300MB - this is about 10\% of the full dataset - we are using a reduced dataset to minimise system requirements and speed up the workflow.

To get started, be sure you are in the ``Terminal'' window. We will be working here for the first step of importing the data, and removing the primers from our data. We can import our data using the \texttt{curl} function, we will then remove the primers using a program called \texttt{cutadapt} which is written in Python.

Make sure when you open the terminal you are in the project directory (and refer to last weeks notes if you need to check how to do this).

Don't switch over to R (the ``Console'' tab in the Binder/RStudio environment) until noted. You can download the required dataset and files by copying and pasting the following commands into your command-line terminal:

\begin{Shaded}
\begin{Highlighting}[]
    \ExtensionTok{curl} \AttributeTok{{-}L} \AttributeTok{{-}o}\NormalTok{ dada2\_amplicon\_ex\_workflow.tar.gz https://ndownloader.figshare.com/files/23066516}
    \FunctionTok{tar} \AttributeTok{{-}xzvf}\NormalTok{ dada2\_amplicon\_ex\_workflow.tar.gz}
    \FunctionTok{rm}\NormalTok{ dada2\_amplicon\_ex\_workflow.tar.gz}
    \BuiltInTok{cd}\NormalTok{ dada2\_amplicon\_ex\_workflow/}
\end{Highlighting}
\end{Shaded}

\textbf{Q. Can you work out what each of these lines of code might be doing? - Click here for Answer}

\emph{In brief these commmands:}

\emph{- download/curl some external data}
\emph{- uncompress into a folder}
\emph{- remove the compressed file}
\emph{- change the working directory to the newly created folder}

In our working directory there are now 20 samples with one forward (R1) and one reverse (R2) read each, each file has DNA sequences with per-base-call quality information, for a total of 40 fastq files (.fq). It is a good idea to have a file with all the sample names to use for various things throughout, so here's making that file based on how these sample names are formatted.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{ls} \PreprocessorTok{*}\NormalTok{\_R1.fq }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}f1} \AttributeTok{{-}d} \StringTok{"\_"} \OperatorTok{\textgreater{}}\NormalTok{ samples}
\end{Highlighting}
\end{Shaded}

\hypertarget{fastq}{%
\subsection{FASTQ?}\label{fastq}}

FASTQ format is a text-based format for storing both a biological sequence (usually nucleotide sequence) and its corresponding quality scores. As each nucleotide in a \emph{read} is sequenced, it is assigned a \emph{Phred quality score}. This score is the assigned \emph{probability} of the sequencer having made an incorrect base call

\begin{longtable}{rll}
\toprule
\textbf{Phred Quality Score} & \textbf{Probability of incorrect base call} & \textbf{Base call accuracy}\\
\midrule
10 & 1 in 10 & 90\%\\
20 & 1 in 100 & 99\%\\
30 & 1 in 1000 & 99.90\%\\
40 & 1 in 10,000 & 99.99\%\\
50 & 1 in 100,000 & 100.00\%\\
\addlinespace
60 & 1 in 1,000,000 & 100.00\%\\
\bottomrule
\end{longtable}

These quality scores are stored within the FASTQ files as ASCII characters

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/ascii} 

}

\caption{Phred quality scores as ASCII characters}\label{fig:unnamed-chunk-40}
\end{figure}

This is all stored together as four simple lines of repeating text so that a FASTQ file containing a single sequence might look like this:

\begin{verbatim}
@SEQ_ID
GATTTGGGGTTCAAAGCAGTATCGATCAAATAGTAAATCCATTTGTTCAACTCACAGTTT
+
!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65
\end{verbatim}

A single FASTQ file may contain millions of sequencing reads. Let's look at the first 40 lines of \emph{one} of our FASTQ files. And check it looks like a standard format.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head} \AttributeTok{{-}40}\NormalTok{ B1\_sub\_F1.fq}
\end{Highlighting}
\end{Shaded}

\textbf{Q. By eye, can you tell whether these few lines look to be of good quality? - Click here for Answer}

\emph{The ASCII characters are repeated every fourth line, most of these reads appear to be G letters or close to this - indicating greater than 99.9\% accuracy - pretty good. Very observant students might have noticed that the end of the reads appear to be of lower quality. More on this later}

\hypertarget{the-pipeline}{%
\section{The Pipeline}\label{the-pipeline}}

This is a very simple overview of the pipeline we will run, some of these steps (especially early ones - are applicable to lots of NGS applications), later on they become more specific to \emph{our} data.

\begin{itemize}
\item
  Import the FASTQ files and demultiplex (this step was done for us)
\item
  Remove adapters and primers (these may be included with our reads, but they are not part of the \emph{natural} DNA sequence)
\item
  Check FASTQ data quality and trim/filter reads accordingly
\item
  Dereplicate (collapse identical sequences and choose a representative)
\item
  Assign ASVs - decide if (non-identical) sequences are similar enough to be considered as from the same species
\item
  Join forward and reverse reads together
\item
  Assign ASVs to Taxonomies
\item
  Count the abundance of different ASVs
\item
  Export taxonomy file, ASV fasta sequence file and count file to R for analysis
\end{itemize}

\hypertarget{removing-primers}{%
\section{Removing Primers}\label{removing-primers}}

To start, we need to remove the primers from all of these (the primers used for this run are in the ``primers.fa'' file in our working directory), and here we're going to use \texttt{cutadapt} to do that at the command line (``Terminal'' tab).

First we need to install \texttt{cutadapt}

\begin{Shaded}
\begin{Highlighting}[]
\ExtensionTok{python3} \AttributeTok{{-}m}\NormalTok{ pip install }\AttributeTok{{-}{-}user} \AttributeTok{{-}{-}upgrade}\NormalTok{ cutadapt}
\end{Highlighting}
\end{Shaded}

\begin{quote}
\textbf{Note}

You will probably get a warning message about PATH. You can ignore this, what it means is that CUTADAPT has been installed in your home directory, in order to use it we need to specify the absolute path TO cutadapt when we call it. This is done for you in the next code box below.
\end{quote}

Cutadapt operates on one sample at at time, so we're going to use a wonderful little bash \emph{loop} to run it on all of our samples.

\hypertarget{loops}{%
\subsection{Loops}\label{loops}}

Loops are extremely powerful way of controlling iteration. We can specify that a line of code is repeated across multiple objects. In this example we use the \texttt{samples} file we made earlier as the list of files across which we want this function of removing primers to loop. These same lines will then repeat until all the specified iterations are complete.

We won't break down exactly how this loop works - but they are used across all programming languages (including R) and you can check out the R4DS book for an introduction to building your own loops (and custom functions!) \href{https://r4ds.had.co.nz/iteration.html}{here}.

For now just copy and paste this code exactly into the Terminal.

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{for}\NormalTok{ sample }\KeywordTok{in} \VariableTok{$(}\FunctionTok{cat}\NormalTok{ samples}\VariableTok{)}
    \ControlFlowTok{do}

        \BuiltInTok{echo} \StringTok{"On sample: }\VariableTok{$sample}\StringTok{"}
        
        \ExtensionTok{\textasciitilde{}/.local/bin/cutadapt} \AttributeTok{{-}a}\NormalTok{ \^{}GTGCCAGCMGCCGCGGTAA...ATTAGAWACCCBDGTAGTCC }\DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}A}\NormalTok{ \^{}GGACTACHVGGGTWTCTAAT...TTACCGCGGCKGCTGGCAC }\DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}m}\NormalTok{ 215 }\AttributeTok{{-}M}\NormalTok{ 285 }\AttributeTok{{-}{-}discard{-}untrimmed} \DataTypeTok{\textbackslash{}}
        \AttributeTok{{-}o} \VariableTok{$\{sample\}}\NormalTok{\_sub\_R1\_trimmed.fq.gz }\AttributeTok{{-}p} \VariableTok{$\{sample\}}\NormalTok{\_sub\_R2\_trimmed.fq.gz }\DataTypeTok{\textbackslash{}}
        \VariableTok{$\{sample\}}\NormalTok{\_sub\_R1.fq }\VariableTok{$\{sample\}}\NormalTok{\_sub\_R2.fq }\DataTypeTok{\textbackslash{}}
        \OperatorTok{\textgreater{}\textgreater{}}\NormalTok{ cutadapt\_primer\_trimming\_stats.txt }\DecValTok{2}\OperatorTok{\textgreater{}\&}\DecValTok{1}

    \ControlFlowTok{done}
\end{Highlighting}
\end{Shaded}

Here's a before-and-after of one of our files - if you look at the sequences supplied: GTGCCAGCMGCCGCGGTAA\ldots ATTAGAWACCCBDGTAGTCC these indicate the forward primer and the reverse primer (our amplicon is everything inbetween). If you look at our before and after you should see these were at the start and end of the sequence but have now been trimmed off.

\begin{verbatim}
### R1 BEFORE TRIMMING PRIMERS
head -n 2 B1_sub_R1.fq
# @M02542:42:000000000-ABVHU:1:1101:8823:2303 1:N:0:3
# GTGCCAGCAGCCGCGGTAATACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTCTTGT
# AAGACAGAGGTGAAATCCCTGGGCTCAACCTAGGAATGGCCTTTGTGACTGCAAGGCTGGAGTGCGGCAGAGGGGGATGG
# AATTCCGCGTGTAGCAGTGAAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAGTCCCCTGGGCCTGCACTGAC
# GCTCATGCACGAAAGCGTGGGGAGCAAACAGGATTAGATACCCGGGTAGTCC

### R1 AFTER TRIMMING PRIMERS
head -n 2 B1_sub_R1_trimmed.fq
# @M02542:42:000000000-ABVHU:1:1101:8823:2303 1:N:0:3
# TACGTAGGGTGCGAGCGTTAATCGGAATTACTGGGCGTAAAGCGTGCGCAGGCGGTCTTGTAAGACAGAGGTGAAATCCC
# TGGGCTCAACCTAGGAATGGCCTTTGTGACTGCAAGGCTGGAGTGCGGCAGAGGGGGATGGAATTCCGCGTGTAGCAGTG
# AAATGCGTAGATATGCGGAGGAACACCGATGGCGAAGGCAGTCCCCTGGGCCTGCACTGACGCTCATGCACGAAAGCGTG
# GGGAGCAAACAGG
\end{verbatim}

You can look through the output of the cutadapt stats file we made (``cutadapt\_primer\_trimming\_stats.txt'') to get an idea of how things went.

Here's a little one-liner to look at what fraction of reads were retained in each sample (column 2) and what fraction of bps were retained in each sample (column 3):

\begin{Shaded}
\begin{Highlighting}[]
    \ExtensionTok{paste}\NormalTok{ samples }\OperatorTok{\textless{}(}\FunctionTok{grep} \StringTok{"passing"}\NormalTok{ cutadapt\_primer\_trimming\_stats.txt }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}f3} \AttributeTok{{-}d} \StringTok{"("} \KeywordTok{|} \FunctionTok{tr} \AttributeTok{{-}d} \StringTok{")"}\OperatorTok{)} \OperatorTok{\textless{}(}\FunctionTok{grep} \StringTok{"filtered"}\NormalTok{ cutadapt\_primer\_trimming\_stats.txt }\KeywordTok{|} \FunctionTok{cut} \AttributeTok{{-}f3} \AttributeTok{{-}d} \StringTok{"("} \KeywordTok{|} \FunctionTok{tr} \AttributeTok{{-}d} \StringTok{")"}\OperatorTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# B1    96.5%   83.0%
# B2    96.6%   83.3%
# B3    95.4%   82.4%
# B4    96.8%   83.4%
# BW1   96.4%   83.0%
# BW2   94.6%   81.6%
# R10   92.4%   79.8%
# R11BF 90.6%   78.2%
# R11   93.3%   80.6%
# R12   94.3%   81.4%
# R1A   93.3%   80.5%
# R1B   94.0%   81.1%
# R2    94.0%   81.2%
# R3    93.8%   81.0%
# R4    95.5%   82.4%
# R5    93.7%   80.9%
# R6    92.7%   80.1%
# R7    94.4%   81.5%
# R8    93.2%   80.4%
# R9    92.4%   79.7%
\end{verbatim}

This looks like it worked pretty well!
Some reads were discarded entirely \texttt{-m\ 215\ -M\ 285\ -\/-discard-untrimmed\ \textbackslash{}} anything \textless215bp or \textgreater285bp was discarded. Looks like c.~6-8\%
We lost a greater proportion of bp overall, but this is the program working as it should, making most of our reads a little shorter as it cuts off the primers. Overall it looks like we lost c.20\% of bp.

Importantly all of our files have behaved \emph{roughly} the same.

With primers removed, we're now ready to switch R and start using DADA2!

\hypertarget{dada2}{%
\section{DADA2}\label{dada2}}

\begin{rmdwarning}
Switch from the Terminal to Console now. We are working in R for the
rest of this workflow
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{library}\NormalTok{(dada2)}

    \FunctionTok{setwd}\NormalTok{(}\StringTok{"dada2\_amplicon\_ex\_workflow"}\NormalTok{)}

    \FunctionTok{list.files}\NormalTok{() }\CommentTok{\# make sure what we think is here is actually here}

    \DocumentationTok{\#\# first we\textquotesingle{}re setting a few R objects we\textquotesingle{}re going to use \#\#}
      \CommentTok{\# one with all sample names, by scanning our "samples" file we made earlier}
\NormalTok{  samples }\OtherTok{\textless{}{-}} \FunctionTok{scan}\NormalTok{(}\StringTok{"samples"}\NormalTok{, }\AttributeTok{what=}\StringTok{"character"}\NormalTok{)}

      \CommentTok{\# one holding the file names of all the forward reads}
\NormalTok{  forward\_reads }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(samples, }\StringTok{"\_sub\_R1\_trimmed.fq.gz"}\NormalTok{)}

      \CommentTok{\# and one with the reverse}
\NormalTok{  reverse\_reads }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(samples, }\StringTok{"\_sub\_R2\_trimmed.fq.gz"}\NormalTok{)}

      \CommentTok{\# and variables holding file names for the forward and reverse}
      \CommentTok{\# filtered reads we\textquotesingle{}re going to generate below}
\NormalTok{  filtered\_forward\_reads }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(samples, }\StringTok{"\_sub\_R1\_filtered.fq.gz"}\NormalTok{)}
\NormalTok{  filtered\_reverse\_reads }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(samples, }\StringTok{"\_sub\_R2\_filtered.fq.gz"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{quality-trimmingfiltering}{%
\section{Quality trimming/filtering}\label{quality-trimmingfiltering}}

We did a filtering step above with cutadapt (where we eliminated reads that had imperfect or missing primers and those that were shorter than 215 bps or longer than 285), but in DADA2 we'll implement a trimming step as well (where we trim reads down based on some quality threshold rather than throwing the read away).

Since we're potentially shortening reads further, we're again going to include another minimum-length filtering component. We can also take advantage of a handy quality plotting function that DADA2 provides to visualize how you're reads are doing, plotQualityProfile().

By running that on our variables that hold all of our forward and reverse read filenames, we can easily generate plots for all samples or for a subset of them. So let's take a peak at that to help decide our trimming lengths: It's good to try to keep a bird's-eye view of what's going on. So here is an overview of the main processing steps we'll be performing with cutadapt and DADA2. Don't worry if anything seems unclear right now, we will discuss each at each step.

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plotQualityProfile}\NormalTok{(forward\_reads[}\DecValTok{17}\SpecialCharTok{:}\DecValTok{20}\NormalTok{])}

    \FunctionTok{plotQualityProfile}\NormalTok{(reverse\_reads[}\DecValTok{17}\SpecialCharTok{:}\DecValTok{20}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

All forwards look pretty similar to each other, and all reverses look pretty similar to each other, but worse than the forwards, which is common --
the Illumina sequencer reads all of the molecules in the forward orientation \emph{first}, then the clusters are flipped and read in reverse. But this means a lot of the chemical reagents start to get used up or degraded, so it is usual to get lower quality reverse reads.

On the plots produced

\begin{itemize}
\item
  the x axis is the nucleotide bases starting from the beginning of the read moving to the end
\item
  the y axis is the average quality score for the base in that position
\item
  the green line is the median quality score of the base at that position
\item
  the orange lines are quartiles
\end{itemize}

These quality profiles are based entirely on taking the \emph{average} PHRED scores for sequences at that position in the sample

Here, I'm going to cut the forward reads at 250 and the reverse reads at 200 -- roughly where both sets maintain a median quality of 30 or above -- and then see how things look. But we also want to set a minimum length to filter out truncated sequences, so we will set a minimum acceptable read length of 175bp (any reads shorter than this will be discarded).

In DADA2, this quality-filtering step is done with the \texttt{filterAndTrim()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    filtered\_out }\OtherTok{\textless{}{-}} \FunctionTok{filterAndTrim}\NormalTok{(forward\_reads, filtered\_forward\_reads,}
\NormalTok{                    reverse\_reads, filtered\_reverse\_reads, }\AttributeTok{maxEE=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{),}
                    \AttributeTok{rm.phix=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minLen=}\DecValTok{175}\NormalTok{, }\AttributeTok{truncLen=}\FunctionTok{c}\NormalTok{(}\DecValTok{250}\NormalTok{,}\DecValTok{200}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This function made a bunch of output files ``filtered\_forward\_reads'' and ``filtered\_reverse\_reads'' we can see these in our project pane. Or if we were working on a server without a GUI we could use \texttt{list.files()} in R or \texttt{ls} in our Terminal.

We also generated an R file called filtered\_out. This is a simple matrix holding how many reads went \emph{in} for each file and how many came \emph{out}.

Check it in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{    filtered\_out}
\end{Highlighting}
\end{Shaded}

We can take a look at the filtered reads visually - we expect to have trimmed off that section where quality drops

\begin{Shaded}
\begin{Highlighting}[]
    \FunctionTok{plotQualityProfile}\NormalTok{(filtered\_reverse\_reads[}\DecValTok{17}\SpecialCharTok{:}\DecValTok{20}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Looking Good!

\hypertarget{dereplication}{%
\section{Dereplication}\label{dereplication}}

Dereplication is a common step in many amplicon processing workflows. Instead of keeping 100 identical sequences and doing all downstream processing to all 100 -costing computer processing power and time, you can keep/process just one of them, and just attach the number x100 to it. Now this acts as a representative for 100 identical sequences.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{derep\_forward }\OtherTok{\textless{}{-}} \FunctionTok{derepFastq}\NormalTok{(filtered\_forward\_reads, }\AttributeTok{verbose=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{names}\NormalTok{(derep\_forward) }\OtherTok{\textless{}{-}}\NormalTok{ samples }\CommentTok{\# the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow}
\NormalTok{derep\_reverse }\OtherTok{\textless{}{-}} \FunctionTok{derepFastq}\NormalTok{(filtered\_reverse\_reads, }\AttributeTok{verbose=}\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{names}\NormalTok{(derep\_reverse) }\OtherTok{\textless{}{-}}\NormalTok{ samples}
\end{Highlighting}
\end{Shaded}

\hypertarget{asvs}{%
\section{ASV's}\label{asvs}}

This is where we start to take our raw sequence data and infer \emph{true} biological sequences.
It uses an algorithm to look at the consensus quality score and abundance for each \emph{unique} sequence. It then determines whether this sequence is more likely to be of biological origin or a spurious sequencing error.

\begin{quote}
\textbf{Note}

This step may take a few minutes to run, so be patient!
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"amplicon\_dada2\_ex.RData"}\NormalTok{)}

\NormalTok{dada\_forward }\OtherTok{\textless{}{-}} \FunctionTok{dada}\NormalTok{(derep\_forward, }\AttributeTok{err=}\NormalTok{err\_forward\_reads, }\AttributeTok{pool=}\StringTok{"pseudo"}\NormalTok{)}

\NormalTok{dada\_reverse }\OtherTok{\textless{}{-}} \FunctionTok{dada}\NormalTok{(derep\_reverse, }\AttributeTok{err=}\NormalTok{err\_reverse\_reads, }\AttributeTok{pool=}\StringTok{"pseudo"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{merging-reads}{%
\section{Merging reads}\label{merging-reads}}

Now DADA2 merges the forward and reverse ASVs to reconstruct our full target amplicon requiring the overlapping region to be identical between the two. By default it requires that at least 12 bps overlap, but in our case the overlap should be much greater. If you remember above we trimmed the forward reads to 250 and the reverse to 200, and our primers were 515f--806r. After cutting off the primers we're expecting a typical amplicon size of around 260 bases, so our typical overlap should be up around 190. That's estimated based on E. coli 16S rRNA gene positions and very back-of-the-envelope-esque of course, so to allow for true biological variation and such I'm going ot set the minimum overlap for this dataset for 170. I'm also setting the trimOverhang option to TRUE in case any of our reads go passed their opposite primers (which I wouldn't expect based on our trimming, but is possible due to the region and sequencing method).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged\_amplicons }\OtherTok{\textless{}{-}} \FunctionTok{mergePairs}\NormalTok{(dada\_forward, derep\_forward, dada\_reverse,}
\NormalTok{                    derep\_reverse, }\AttributeTok{trimOverhang=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{minOverlap=}\DecValTok{170}\NormalTok{)}

  \CommentTok{\# this object holds a lot of information that may be the first place you\textquotesingle{}d want to look if you want to start poking under the hood}
\FunctionTok{class}\NormalTok{(merged\_amplicons) }\CommentTok{\# list}
\FunctionTok{length}\NormalTok{(merged\_amplicons) }\CommentTok{\# 20 elements in this list, one for each of our samples}
\FunctionTok{names}\NormalTok{(merged\_amplicons) }\CommentTok{\# the names() function gives us the name of each element of the list }

\FunctionTok{class}\NormalTok{(merged\_amplicons}\SpecialCharTok{$}\NormalTok{B1) }\CommentTok{\# each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe}

\FunctionTok{names}\NormalTok{(merged\_amplicons}\SpecialCharTok{$}\NormalTok{B1) }\CommentTok{\# the names() function on a dataframe gives you the column names}
\CommentTok{\# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"}
\end{Highlighting}
\end{Shaded}

\hypertarget{count-table}{%
\section{Count table}\label{count-table}}

Now we can generate a count table with the makeSequenceTable() function. This is one of the main outputs from processing an amplicon dataset. You may have also heard this referred to as a biome table, or an OTU matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{seqtab }\OtherTok{\textless{}{-}} \FunctionTok{makeSequenceTable}\NormalTok{(merged\_amplicons)}
\FunctionTok{class}\NormalTok{(seqtab) }\CommentTok{\# matrix}
\FunctionTok{dim}\NormalTok{(seqtab) }\CommentTok{\# 20 2521}
\end{Highlighting}
\end{Shaded}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

The developers' DADA2 tutorial provides an example of a nice, quick way to pull out how many reads were dropped at various points of the pipeline. This can serve as a jumping off point if you're left with too few sequences at the end to help point you towards where you should start digging into where they are being dropped. Here's a slightly modified version:

\begin{Shaded}
\begin{Highlighting}[]
  \CommentTok{\# set a little function}
\NormalTok{getN }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sum}\NormalTok{(}\FunctionTok{getUniques}\NormalTok{(x))}

  \CommentTok{\# making a little table}
\NormalTok{summary\_tab }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{row.names=}\NormalTok{samples, }\AttributeTok{dada2\_input=}\NormalTok{filtered\_out[,}\DecValTok{1}\NormalTok{],}
               \AttributeTok{filtered=}\NormalTok{filtered\_out[,}\DecValTok{2}\NormalTok{], }\AttributeTok{dada\_f=}\FunctionTok{sapply}\NormalTok{(dada\_forward, getN),}
               \AttributeTok{dada\_r=}\FunctionTok{sapply}\NormalTok{(dada\_reverse, getN), }\AttributeTok{merged=}\FunctionTok{sapply}\NormalTok{(merged\_amplicons, getN),}
               \AttributeTok{nonchim=}\FunctionTok{rowSums}\NormalTok{(seqtab.nochim),}
               \AttributeTok{final\_perc\_reads\_retained=}\FunctionTok{round}\NormalTok{(}\FunctionTok{rowSums}\NormalTok{(seqtab.nochim)}\SpecialCharTok{/}\NormalTok{filtered\_out[,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{*}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{))}

\NormalTok{summary\_tab}
\end{Highlighting}
\end{Shaded}

And it might be useful to write this table out of R, saving it as a regular file

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(summary\_tab, }\StringTok{"read{-}count{-}tracking.tsv"}\NormalTok{, }\AttributeTok{quote=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{sep=}\StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\AttributeTok{col.names=}\ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{assign-taxonomy}{%
\section{Assign taxonomy}\label{assign-taxonomy}}

\begin{rmdwarning}
Running the Taxonomy assignment step below can take anywhere from 30
minutes to a few hours depending on how much RAM we provide. So for this
example run - we will skip this step and load an R.data file which has
this information in it already
\end{rmdwarning}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{load}\NormalTok{(}\StringTok{"amplicon\_dada2\_ex.RData"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Example code for running taxonomy assignment - Click here}

\emph{So we won't run this code in this example, but here it is for reference.}

\begin{verbatim}
## downloading DECIPHER-formatted SILVA v138 reference
download.file(url="http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData", destfile="SILVA_SSU_r138_2019.RData")

## loading reference taxonomy object
load("SILVA_SSU_r138_2019.RData")

## loading DECIPHER
library(DECIPHER)

## creating DNAStringSet object of our ASVs
dna <- DNAStringSet(getSequences(seqtab.nochim))

## and classifying
tax_info <- IdTaxa(test=dna, trainingSet=trainingSet, strand="both", processors=NULL)
\end{verbatim}

\hypertarget{standard-goods}{%
\section{Standard goods}\label{standard-goods}}

The typical standard outputs from amplicon processing are

\begin{itemize}
\item
  a fasta file: each ASV represented by a sequence \texttt{asv\_fasta\_no\_contam}
\item
  a count table: how many sequences of each ASV in each sample \texttt{asv\_tab\_no\_contam}
\item
  a taxonomy file: the closest biological species to the fasta sequence \texttt{asv\_tax\_no\_contam}
\end{itemize}

These objects from DADA2 can then be analysed to start to understand the different bacterial communities from our deep-sea study:

\begin{quote}
\textbf{Note}

\begin{itemize}
\tightlist
\item
  These three files are relatively small simply lists now, you can type them into the R console and inspect these outputs if you wish. Do they make sense to you?
\end{itemize}
\end{quote}

\hypertarget{functions-list}{%
\section{Functions list}\label{functions-list}}

\begin{longtable}{ll}
\toprule
\textbf{Command} & \textbf{What it is}\\
\midrule
cutadapt/filterAndTrim() & remove primers and quality trim/filter\\
learnErrors() & generate an error model of our data\\
derepFastq & dereplicate sequences\\
dada() & infer ASVs on both forward and reverse reads independently\\
mergePairs() & merge forward and reverse reads to further refine ASVs\\
\addlinespace
makeSequenceTable() & generate a count table\\
removeBimeraDenovo() & screen for and remove chimeras\\
IdTaxa() & assign taxonomy\\
\bottomrule
\end{longtable}

\hypertarget{summary-2}{%
\section{Summary}\label{summary-2}}

We have imported FASTQ data from an Illumina sequencing run, processed the files to remove poor quality reads and trim primers. We have then put this through a microbiome specific bioinformatics pipeline to assign millions of individual reads to more manageable representative sequences. We have assigned taxonomies to these sequences and tallied them, so that \textbf{next time} we can actually inspect our data and start to make visuals that describe our microbial communities.

\hypertarget{introduction-to-statistical-analysis-of-the-microbiome-data}{%
\chapter{Introduction to Statistical Analysis of the microbiome data}\label{introduction-to-statistical-analysis-of-the-microbiome-data}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(phyloseq)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(gt)}
\end{Highlighting}
\end{Shaded}

This section provides a quick introduction to some common analytic methods used to analyze microbiome data.
In previous sections we worked with microbiome generated data, but ran through some fairly standard protocols that would apply to any Illumina-seq generated data and FASTQ files.

Now we are at the visualisation and statistical analysis phase - and we should start to see the implementation of some very familiar R tools.

Briefly today we will use R to understand more about

\begin{itemize}
\item
  Describing the microbial community composition of a set of samples
\item
  Estimating within- and between-sample diversity
\item
  Identifying differentially abundant taxa
\end{itemize}

\hypertarget{important-files}{%
\section{Important files}\label{important-files}}

We're mostly going to be working with just 3 files now.

\begin{itemize}
\item
  A count table: the number of reads for each unique sequence
\item
  A taxonomy table: the assigned taxonomy for each sequence according to the SILVA database
\item
  A sample file: this is the ``metadata'' it contains any information \emph{we} have provided about the different samples
\end{itemize}

If you worked through the entirety of the previous session you will already have the necessary R objects in your workspace. If you didn't we can load them using a \texttt{.RData} file. You should see that your Environment is suddenly populated by a number of R objects.

The one thing missing that we also need to load is a sample info object (it tells us meta-data about the origins and conditions of each environmental sample.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_info\_tab }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{"sample\_info.tsv"}\NormalTok{, }\AttributeTok{header=}\NormalTok{T, }\AttributeTok{row.names=}\DecValTok{1}\NormalTok{,}
                   \AttributeTok{check.names=}\NormalTok{F, }\AttributeTok{sep=}\StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{phyloseq}{%
\section{Phyloseq}\label{phyloseq}}

Microbiome community analysis works really well with a package called \texttt{phyloseq} it allows you to make special R objects that hold the distinct sets of information on ASV abundance, taxonomy and relation to the environmental samples in one R object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps }\OtherTok{\textless{}{-}} \FunctionTok{phyloseq}\NormalTok{(}\FunctionTok{otu\_table}\NormalTok{(asv\_tab\_no\_contam, }\AttributeTok{taxa\_are\_rows=}\NormalTok{T), }
               \FunctionTok{sample\_data}\NormalTok{(sample\_info\_tab), }
               \FunctionTok{tax\_table}\NormalTok{(asv\_tax\_no\_contam))}
\NormalTok{ps}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## phyloseq-class experiment-level object
## otu_table()   OTU Table:         [ 2498 taxa and 16 samples ]
## sample_data() Sample Data:       [ 16 samples by 4 sample variables ]
## tax_table()   Taxonomy Table:    [ 2498 taxa by 7 taxonomic ranks ]
\end{verbatim}

\begin{quote}
\textbf{Note}

Since we've already used decontam to remove likely contaminants in the previous steps, we're dropping the ``blank'' samples from our count table - so now there are 16 samples total.
\end{quote}

\textbf{Take a look at each of the three objects: asv\_tab\_no\_contam, sample\_data, tax\_table. There are just simple lists - can you comfortably work out the information they contain?}

\begin{itemize}
\item
  \texttt{asv\_tab\_no\_contam} A count table: the number of reads for each unique sequence
\item
  \texttt{asv\_tax\_no\_contam} A taxonomy table: the assigned taxonomy for each sequence according to the \href{https://www.arb-silva.de/}{SILVA} database
\item
  \texttt{sample\_info\_tab} A sample file: this is the ``metadata'' it contains any information \emph{we} have provided about the different samples
\end{itemize}

\hypertarget{relative-abundance-of-bacteria-by-taxonomic-class}{%
\section{Relative abundance of bacteria by taxonomic Class}\label{relative-abundance-of-bacteria-by-taxonomic-class}}

We will use some functions provided by \texttt{phyloseq} to agglomerate the reads to Class-level and plot the results

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taxonomy }\OtherTok{\textless{}{-}}\NormalTok{ ps }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tax\_glom}\NormalTok{(}\AttributeTok{taxrank=}\StringTok{"class"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\#\# Preserves taxonomy only to Taxonomic level class}
  \FunctionTok{transform\_sample\_counts}\NormalTok{(}\ControlFlowTok{function}\NormalTok{(x)\{x}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x)\})}\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\#\# this function turns Abundance from raw counts to a relative proportion}
  \FunctionTok{psmelt}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\#\#converts phyloseq object into a tibble }
  \FunctionTok{filter}\NormalTok{(Abundance }\SpecialCharTok{\textgreater{}}\FloatTok{0.05}\NormalTok{) }\DocumentationTok{\#\#\# remove any ASVs at less than 5\% abundance}
\end{Highlighting}
\end{Shaded}

Often an early step in many microbiome projects to visualize the relative abundance of organisms at specific taxonomic ranks.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taxonomy }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Sample, }\AttributeTok{y=}\NormalTok{Abundance, }\AttributeTok{fill=}\NormalTok{class))}\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-67-1.pdf}
The ability to discriminate between more than say a dozen colors in a single plot is a limitation of the stacked bar plot - so we have filtered out less abundant reads (\textless5\%). We run the risk that we might miss something important here, so its important to visualise the data in different ways.

There are a total of 10 Bacterial classes (at greater than 5\% abundance).

Let's generate boxplots according to \texttt{char} and facet these according to Bacterial \texttt{class}. This will allow us to compare how different bacterial classes appear to change in relative abundance between samples.

Try and produce something similar to the plot below

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-68-1.pdf}

Think about what you can infer from these plots?

\hypertarget{alpha-diversity---community-richness}{%
\section{Alpha diversity - community richness}\label{alpha-diversity---community-richness}}

This is a common ecological term, and one that applies just as well when we consider microbial ecosystems.

What is the mean species diversity in the different sample sites?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps\_richness }\OtherTok{\textless{}{-}} \FunctionTok{estimate\_richness}\NormalTok{(ps)}
\NormalTok{ps\_richness}
\end{Highlighting}
\end{Shaded}

This command generates a number of different diversity indices.
Firstly we can take this information and use \texttt{join} functions to attach it back onto our sample \emph{metadata}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_info\_tab}\SpecialCharTok{$}\NormalTok{Sample }\OtherTok{\textless{}{-}} \FunctionTok{rownames}\NormalTok{(sample\_info\_tab)}

\NormalTok{ps\_richness}\SpecialCharTok{$}\NormalTok{Sample }\OtherTok{\textless{}{-}} \FunctionTok{rownames}\NormalTok{(ps\_richness)}

\NormalTok{full\_richness }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(sample\_info\_tab, ps\_richness, }\StringTok{"Sample"}\NormalTok{)}
\NormalTok{full\_richness }
\end{Highlighting}
\end{Shaded}

\captionsetup[table]{labelformat=empty,skip=1pt}
\begin{longtable}{rccclrrrrrrrrr}
\toprule
temp & type & char & color & Sample & Observed & Chao1 & se.chao1 & ACE & se.ACE & Shannon & Simpson & InvSimpson & Fisher \\ 
\midrule
2.0 & water & water & blue & BW1 & 61 & 62.5000 & 2.5911387 & 61.88289 & 3.569938 & 3.576981 & 0.9585698 & 24.13700 & 12.04096 \\ 
2.0 & water & water & blue & BW2 & 204 & 205.9091 & 2.1571603 & 205.71458 & 7.069400 & 4.436499 & 0.9750331 & 40.05305 & 43.45043 \\ 
13.7 & rock & glassy & black & R10 & 383 & 385.4375 & 2.1449492 & 386.66260 & 9.717693 & 5.079826 & 0.9872712 & 78.56183 & 81.57801 \\ 
7.3 & biofilm & biofilm & darkgreen & R11BF & 144 & 145.1538 & 1.5236935 & 146.00669 & 5.980036 & 3.309788 & 0.9044015 & 10.46042 & 25.68876 \\ 
7.3 & rock & glassy & black & R11 & 281 & 284.4737 & 2.9104269 & 284.57048 & 8.337302 & 4.812376 & 0.9851114 & 67.16529 & 59.63308 \\ 
NA & rock & altered & chocolate4 & R12 & 524 & 525.8947 & 1.9684441 & 525.94449 & 11.362166 & 5.789071 & 0.9956479 & 229.77212 & 115.57917 \\ 
8.6 & rock & altered & chocolate4 & R1A & 497 & 501.2500 & 2.9984998 & 502.32876 & 11.107301 & 5.666947 & 0.9945360 & 183.01554 & 115.04680 \\ 
8.6 & rock & altered & chocolate4 & R1B & 577 & 579.6000 & 2.2526890 & 580.24471 & 11.965009 & 5.812676 & 0.9953007 & 212.79743 & 129.04718 \\ 
8.6 & rock & altered & chocolate4 & R2 & 608 & 609.4062 & 1.5321223 & 610.17688 & 12.296746 & 5.850938 & 0.9955702 & 225.74143 & 133.85232 \\ 
12.7 & rock & altered & chocolate4 & R3 & 566 & 567.5517 & 1.6446664 & 568.36545 & 11.819663 & 5.788769 & 0.9953827 & 216.57487 & 122.16253 \\ 
12.7 & rock & altered & chocolate4 & R4 & 643 & 644.2857 & 1.4737632 & 644.75628 & 12.645710 & 5.718854 & 0.9940234 & 167.31991 & 137.19651 \\ 
12.7 & rock & altered & chocolate4 & R5 & 624 & 624.2069 & 0.5163104 & 624.78554 & 12.367463 & 5.966462 & 0.9962834 & 269.06381 & 137.21126 \\ 
12.7 & rock & altered & chocolate4 & R6 & 523 & 527.2857 & 3.1338410 & 527.52112 & 11.340777 & 5.711816 & 0.9948403 & 193.80933 & 115.90757 \\ 
NA & rock & carbonate & darkkhaki & R7 & 435 & 436.7838 & 1.7397113 & 438.10995 & 10.011218 & 5.557851 & 0.9939838 & 166.21652 & 110.12229 \\ 
13.5 & rock & glassy & black & R8 & 424 & 428.1379 & 3.0460370 & 428.53605 & 10.245312 & 5.295582 & 0.9910943 & 112.28782 & 91.16232 \\ 
13.7 & rock & glassy & black & R9 & 296 & 300.5652 & 3.3753169 & 300.44067 & 8.397187 & 4.348445 & 0.9517400 & 20.72112 & 63.39469 \\ 
\bottomrule
\end{longtable}

\hypertarget{analyse-and-plot}{%
\subsection{Analyse and plot}\label{analyse-and-plot}}

We now have a tidy data frame with sample information and a variety of diversity indices.

\begin{itemize}
\item
  Using the Shannon index as the measure of diversity - can you construct a figure to show the diversity differences between treatments?
\item
  Again using the Shannon index - can you construct a \texttt{general\ linear\ model} to work out which treatments/effects might be having a significant effect on diversity?
\end{itemize}

\hypertarget{beta-diversity}{%
\section{Beta diversity}\label{beta-diversity}}

\hypertarget{heatmaps}{%
\subsection{Heatmaps}\label{heatmaps}}

Heatmaps are basically false colour images where cells in the matrix with high relative values are coloured differently from those with low relative values. Heatmaps can range from very simple blocks of colour with lists along 2 sides, or they can include information about hierarchical clustering, and/or values of other covariates of interest.

We will make a simple heatmap here, but a common addition might be a dendrogram - a phylogenetic tree to indicate how closely related different

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps\_top20 }\OtherTok{\textless{}{-}} \FunctionTok{prune\_taxa}\NormalTok{(}\FunctionTok{names}\NormalTok{(}\FunctionTok{sort}\NormalTok{(}\FunctionTok{taxa\_sums}\NormalTok{(ps),}\ConstantTok{TRUE}\NormalTok{)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{]), ps) }\DocumentationTok{\#\# subset to include only the top 20 most abundant ASVs}

\FunctionTok{plot\_heatmap}\NormalTok{(ps\_top20,}\StringTok{"MDS"}\NormalTok{,}\AttributeTok{distance =} \StringTok{"bray"}\NormalTok{, }\DocumentationTok{\#\# here we are setting up the parameters for the heat map}
             \AttributeTok{sample.label=}\StringTok{"char"}\NormalTok{, }\DocumentationTok{\#\# set the x axis to display by sample}
             \AttributeTok{low=}\StringTok{"purple"}\NormalTok{,   }\DocumentationTok{\#\# set the color for low abundance OTUs}
             \AttributeTok{high=}\StringTok{"darkorange"}\NormalTok{) }\DocumentationTok{\#\# set the color for high abundance OTUs}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-73-1.pdf}

\hypertarget{multidimensional-analysis}{%
\subsection{Multidimensional analysis}\label{multidimensional-analysis}}

Often in ecological research, we are interested not only in comparing univariate descriptors of communities, like diversity (above), but also in how the constituent species --- or the composition --- changes from one community to the next.

One common tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites, etc.) into just a few, so that they can be visualized and interpreted.

Consider a single axis representing the abundance of a single species. Along this axis, we can plot the communities in which this species appears, based on its abundance within each.

Now consider a second axis of abundance, representing another species. We can now plot each community along the two axes (Species 1 and Species 2).

Keep going and imagine how many axes we would have to produce!!!!

The goal of NMDS is to represent the original position of communities in multidimensional space as accurately as possible using a reduced number of dimensions that can be easily plotted and visualized (and to spare your thinker).

NMDS does not use the absolute abundances of species in communities, but rather their rank orders. The use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation), and as a result is a flexible technique

\begin{Shaded}
\begin{Highlighting}[]
    \DocumentationTok{\#\#\# Ordination using built in functions in phyoseq (calls vegan)    }
\NormalTok{ord.nmds.bray }\OtherTok{\textless{}{-}} \FunctionTok{ordinate}\NormalTok{(ps, }\AttributeTok{method=}\StringTok{"NMDS"}\NormalTok{,}\AttributeTok{k=}\DecValTok{2}\NormalTok{, }\AttributeTok{distance=}\StringTok{"bray"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Square root transformation
## Wisconsin double standardization
## Run 0 stress 0.04869473 
## Run 1 stress 0.1099067 
## Run 2 stress 0.0495053 
## Run 3 stress 0.04788148 
## ... New best solution
## ... Procrustes: rmse 0.01819983  max resid 0.05251424 
## Run 4 stress 0.05216696 
## Run 5 stress 0.04950527 
## Run 6 stress 0.05218153 
## Run 7 stress 0.04789994 
## ... Procrustes: rmse 0.003538871  max resid 0.01139184 
## Run 8 stress 0.04346718 
## ... New best solution
## ... Procrustes: rmse 0.05055473  max resid 0.1276931 
## Run 9 stress 0.04346707 
## ... New best solution
## ... Procrustes: rmse 0.0008269613  max resid 0.002619479 
## ... Similar to previous best
## Run 10 stress 0.05221919 
## Run 11 stress 0.05217172 
## Run 12 stress 0.04346703 
## ... New best solution
## ... Procrustes: rmse 4.244008e-05  max resid 0.0001341823 
## ... Similar to previous best
## Run 13 stress 0.04346731 
## ... Procrustes: rmse 0.0008768875  max resid 0.002778816 
## ... Similar to previous best
## Run 14 stress 0.04866373 
## Run 15 stress 0.04788183 
## Run 16 stress 0.04866042 
## Run 17 stress 0.04346729 
## ... Procrustes: rmse 0.0008654347  max resid 0.002742929 
## ... Similar to previous best
## Run 18 stress 0.04788425 
## Run 19 stress 0.0521657 
## Run 20 stress 0.05216982 
## *** Solution reached
\end{verbatim}

You should see each iteration of the NMDS until a solution is reached (i.e., stress was minimized after some number of reconfigurations of the points in 2 dimensions). You can increase the number of default iterations using the argument trymax=. which may help alleviate issues of non-convergence. If high stress is your problem, increasing the number of dimensions to k=3 might also help.

Often in ecological research, we are interested not only in comparing univariate descriptors of communities, like diversity (above), but also in how the constituent species --- or the composition --- changes from one community to the next.

One common tool to do this is non-metric multidimensional scaling, or NMDS. The goal of NMDS is to collapse information from multiple dimensions (e.g, from multiple communities, sites, etc.) into just a few, so that they can be visualized and interpreted.

Consider a single axis representing the abundance of a single species. Along this axis, we can plot the communities in which this species appears, based on its abundance within each.

Now consider a second axis of abundance, representing another species. We can now plot each community along the two axes (Species 1 and Species 2).

Keep going and imagine how many axes we would have to produce!!!!

The goal of NMDS is to represent the original position of communities in multidimensional space as accurately as possible using a reduced number of dimensions that can be easily plotted and visualized (and to spare your thinker).

NMDS does not use the absolute abundances of species in communities, but rather their rank orders. The use of ranks omits some of the issues associated with using absolute distance (e.g., sensitivity to transformation), and as a result is a flexible technique

We can use the function plot.ordination to use our NMDS scaling to position our samples on a ggplot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ord1}\OtherTok{\textless{}{-}}\FunctionTok{plot\_ordination}\NormalTok{(ps, ord.nmds.bray, }\AttributeTok{color=}\StringTok{"char"}\NormalTok{, }\AttributeTok{title=}\StringTok{"Bray NMDS"}\NormalTok{)}
\NormalTok{ord1}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-75-1.pdf}

And we can add customisations to improve the plot

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ord1 }\SpecialCharTok{+} \FunctionTok{stat\_ellipse}\NormalTok{(}\AttributeTok{type=}\StringTok{"norm"}\NormalTok{,}
                    \AttributeTok{geom=}\StringTok{"polygon"}\NormalTok{,}
                    \FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{char),}
                    \AttributeTok{alpha=}\FloatTok{0.3}\NormalTok{,}
                    \AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
     \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{pch=}\DecValTok{21}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{char), }\AttributeTok{colour=}\StringTok{"white"}\NormalTok{, }\AttributeTok{stroke=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{size=}\DecValTok{3}\NormalTok{)}\SpecialCharTok{+}
        \FunctionTok{theme\_classic}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-76-1.pdf}

\hypertarget{question}{%
\section{Question}\label{question}}

We have now produced several different visualisations and analyses of the microbiome communities from these deep sea samples.
What do you think are the most important or interesting findings here?

\hypertarget{summary-3}{%
\section{Summary}\label{summary-3}}

This is a \emph{very} brief introduction to using \textbf{big} data. Hopefully you can see that many of the command line tools, data cleaning and analysis and visualisation you have been practising prepare you for Big Data Biology.

In this example, in order to have generated and processed this data, you would have needed to know

\begin{itemize}
\tightlist
\item
  Sampling techniques
\item
  Molecular lab skills
\item
  Data analysis and coding knowledge
\item
  Ecological skills (community analysis)
\end{itemize}

Biology is increasingly interdisciplinary - and the skills you learn in one module should be transferable to other things you do.

And remember at the heart of everything is Data! It doesn't matter what you are studying or why - good data skills are fundamental.

Thank you!

\hypertarget{applications}{%
\chapter{Applications}\label{applications}}

Some \emph{significant} applications are demonstrated in this chapter.

\hypertarget{example-one}{%
\section{Example one}\label{example-one}}

\hypertarget{example-two}{%
\section{Example two}\label{example-two}}

\hypertarget{final-words}{%
\chapter{Final Words}\label{final-words}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_app}\NormalTok{(}\StringTok{\textquotesingle{}https://yihui.shinyapps.io/miniUI/\textquotesingle{}}\NormalTok{, }\AttributeTok{height =} \StringTok{\textquotesingle{}600px\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \href{https://yihui.shinyapps.io/miniUI/}{\includegraphics{bookdown-demo_files/figure-latex/miniUI-1} }

}

\caption{A Shiny app created via the miniUI package; you can see a live version at https://yihui.shinyapps.io/miniUI/.}\label{fig:miniUI}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_app}\NormalTok{(}\StringTok{\textquotesingle{}https://biouea.shinyapps.io/covidhm\_shiny/?\_ga=2.167137790.1196997618.1629188729{-}590961367.1629188729\%27\textquotesingle{}}\NormalTok{, }\AttributeTok{height =} \StringTok{\textquotesingle{}600px\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\href{https://biouea.shinyapps.io/covidhm_shiny/?_ga=2.167137790.1196997618.1629188729-590961367.1629188729%27}{\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-77-1} }\end{center}

  \bibliography{book.bib,packages.bib}

\end{document}
